{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U --quiet langchain-community tiktoken langchain-openai langchainhub chromadb langchain langgraph langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(key: str):\n",
    "    if key not in os.environ:\n",
    "        os.environ[key] = getpass.getpass(f\"{key}:\")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:43:06.378504Z",
     "start_time": "2024-10-21T21:43:05.655172Z"
    }
   },
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader, PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load materials"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:43:12.139919Z",
     "start_time": "2024-10-21T21:43:11.564437Z"
    }
   },
   "source": [
    "file_path = './assets-resources/pdfs/human-agent-collab-problem-solving.pdf'\n",
    "\n",
    "docs = PyPDFLoader(file_path).load_and_split()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:44:00.321668Z",
     "start_time": "2024-10-21T21:44:00.317758Z"
    }
   },
   "source": [
    "len(docs)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:44:33.877594Z",
     "start_time": "2024-10-21T21:44:33.873487Z"
    }
   },
   "source": "docs[:5]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './assets-resources/pdfs/human-agent-collab-problem-solving.pdf', 'page': 0}, page_content='Large Language Model-based Human-Agent Collaboration\\nfor Complex Task Solving\\nXueyang Feng1,2∗, Zhi-Yuan Chen1,2∗, Yujia Qin3, Yankai Lin1,2†\\nXu Chen1,2†, Zhiyuan Liu3, Ji-Rong Wen1,2\\n1Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China\\n2Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China\\n3Department of Computer Science and Technology, Tsinghua University, Beijing, China\\n{xueyangfeng, zhiyuanc2001, yankailin, xu.chen}@ruc.edu.cn\\nAbstract\\nIn recent developments within the research\\ncommunity, the integration of Large Language\\nModels (LLMs) in creating fully autonomous\\nagents has garnered significant interest. De-\\nspite this, LLM-based agents frequently demon-\\nstrate notable shortcomings in adjusting to dy-\\nnamic environments and fully grasping hu-\\nman needs. In this work, we introduce the\\nproblem of LLM-based human-agent collab-\\noration for complex task-solving, exploring\\ntheir synergistic potential. In addition, we pro-\\npose a Reinforcement Learning-based Human-\\nAgent Collaboration method, ReHAC . This\\napproach includes a policy model designed to\\ndetermine the most opportune stages for hu-\\nman intervention within the task-solving pro-\\ncess. We construct a human-agent collabo-\\nration dataset to train this policy model in\\nan offline reinforcement learning environment.\\nOur validation tests confirm the model’s ef-\\nfectiveness. The results demonstrate that the\\nsynergistic efforts of humans and LLM-based\\nagents significantly improve performance in\\ncomplex tasks, primarily through well-planned,\\nlimited human intervention. Datasets and code\\nare available at: https://github.com/\\nXueyangFeng/ReHAC .\\n1 Introduction\\nIn today’s increasingly complex world, humans are\\nconfronted with multifaceted tasks stemming from\\ntechnical, social, and economic domains. Solv-\\ning these complex tasks necessitates not only hu-\\nman interaction with the environment but also in-\\ntricate decision-making processes. To alleviate\\nhuman workload and enhance the automation of\\ntasks in both professional and personal spheres, re-\\nsearchers have been actively developing advanced\\ntools for human assistance (Zawacki-Richter et al.,\\n∗Equal Contribution. The order is determined by dice\\nrolling.\\n†Corresponding Authors.\\nAgent Human Env\\nAllocator EnvEnv\\n(a)\\nAct Act(b)\\n(c)Agent\\nHumanAct\\nAct\\nI need  helpFigure 1: Different Levels of Automation. (a) No au-\\ntomation: Tasks are entirely performed by humans. (b)\\nFull automation: Tasks are completely executed by\\nagents without human intervention. (c) Conditional\\nautomation: Humans are required only for specific sub-\\ntasks, without continuous monitoring.\\n2019; Amershi et al., 2019). Recently, the emer-\\ngence of Large Language Models (LLMs) such\\nas LLaMA (Touvron et al., 2023), Gemini (Team\\net al., 2023) and GPT (Brown et al., 2020; Achiam\\net al., 2023) has marked a significant milestone.\\nLLMs’ remarkable abilities in task understanding,\\nplanning, and reasoning (Zhao et al., 2023b) have\\ngiven rise to the development of LLM-based au-\\ntonomous agents (Wang et al., 2023a; Yao et al.,\\n2022; Shinn et al., 2023). These agents are de-\\nsigned to leverage the LLMs’ capabilities to assist\\nhumans in solving complex tasks autonomously.\\nThe LLMs’ capabilities enable them to effectively\\nnavigate and address the complexities encountered\\nin real-world scenarios, thereby offering substan-\\ntial support in human decision-making processes\\nof task-solving.\\nDespite the remarkable progress of LLM-based\\nagents, there remains a notable gap in their intelli-\\ngence level to handle complex and dynamic real-\\nworld tasks with human-like proficiency. This limi-\\ntation poses a significant challenge to their practi-\\ncality in real-world applications, especially in sce-arXiv:2402.12914v1  [cs.CL]  20 Feb 2024'),\n",
       " Document(metadata={'source': './assets-resources/pdfs/human-agent-collab-problem-solving.pdf', 'page': 1}, page_content='narios where high accuracy is crucial, such as the\\nlegal or financial domains. Addressing this chal-\\nlenge extends beyond just enhancing the agents’\\ncapabilities. Incorporating human intuition and\\nwisdom is equally vital for the effective manage-\\nment of these intricate and evolving tasks, offering\\na complementary approach to the limitations of\\ncurrent agent technologies.\\nIn this work, we introduce the problem of LLM-\\nbased human-agent collaboration for complex\\ntask solving , aiming to augment the capabilities of\\nLLM-based agents by integrating human intuition\\nand wisdom. The idea is analogous to the evolu-\\ntion in autonomous driving technology, which has\\nbeen categorized into varying levels of autonomy,\\nranging from no automation, conditional automa-\\ntion to full automation (Khan et al., 2022; SAE\\nInternational, 2021). Referring to this framework,\\nwe define the different levels of human-agent col-\\nlaboration, as illustrated in Figure 1. Applying\\nthis conditional automation mode to LLM-based\\nagents offers a practical path for their deployment\\nin real-world scenarios, acknowledging the current\\nlimitations in their cognitive capabilities. Instead\\nof aiming for full automation, human-agent col-\\nlaboration under the paradigm of conditional au-\\ntomation enables humans to intervene the complex\\ntask-solving when necessary, while agents handle\\nmost of the sub-tasks. This takes advantage of both\\nhuman and machine intelligence.\\nWhile advancements in LLMs significantly en-\\nhance the capacity for mutual understanding in\\nhuman-agent collaboration, several crucial chal-\\nlenges persist. These challenges include defining\\nthe division of labor between humans and agents,\\ndetermining the granularity of tool execution, man-\\naging proactive interruption, and implementing\\nmulti-level intervention. However, our research\\nspecifically focuses on scenarios where humans\\ndirectly replace agents in action. The key chal-\\nlenge we aim to address in human-agent collab-\\noration lies in determining the optimal stages for\\nhuman intervention in task-solving and minimiz-\\ning such intervention to enhance efficiency. Some\\nresearchers have made preliminary attempts, by de-\\nsigning heuristic rules or specialized prompts to\\ndetermine the stages at which agents should seek\\nhuman assistance (Cai et al., 2023; Wu et al., 2022a;\\nMehta et al., 2023; Wang et al., 2023b). However,\\nthese rule-based or prompt-driven approaches are\\nheavily reliant on specific application contexts andlack universality. They often demand a deep under-\\nstanding of the domain and substantial experience\\nfrom the designers, otherwise, suboptimal design\\nchoices can lead to reduced performance. Apart\\nfrom that, a standardized formal framework and\\nuniversally accepted paradigm for leveraging large\\nlanguage models (LLMs) in human-agent collabo-\\nration is still lacking.\\nTo overcome the aforementioned challenges, we\\npropose a Reinforcement Learning-based Human-\\nAgent Collaboration method, ReHAC , aimed at\\neffectively combining human intervention with the\\nautomation capabilities of LLM-based agents. Our\\nmethod, leveraging reinforcement learning, trains\\na policy model to dynamically identify the most\\nadvantageous moments for human input during the\\ntask-solving process. ReHAC is a learnable gen-\\neral framework that can be applied to various sce-\\nnarios and does not require additional prior knowl-\\nedge to design rules and prompts. For training\\nthis policy model, we collect a dataset compris-\\ning tasks collaboratively completed by humans and\\nLLM-based agents, utilized for the offline training\\nof the policy model. We conducted extensive ex-\\nperiments on three multi-step reasoning datasets:\\nHotpotQA, StrategyQA, and InterCode, using two\\npopular LLM-based agent frameworks, ReAct and\\n\"Try-again\". The experimental results indicate that\\nwith a policy model learned from limited data, Re-\\nHAC can effectively allocate human intervention\\nin human-agent collaboration scenarios, thereby'),\n",
       " Document(metadata={'source': './assets-resources/pdfs/human-agent-collab-problem-solving.pdf', 'page': 1}, page_content='\"Try-again\". The experimental results indicate that\\nwith a policy model learned from limited data, Re-\\nHAC can effectively allocate human intervention\\nin human-agent collaboration scenarios, thereby\\nachieving a balance between effectiveness and effi-\\nciency.\\n2 Approach\\nIn this section, we first formulate the problem\\nof human-agent collaboration for complex task\\nsolving, and then introduce our proposed ReHAC\\nmethod in detail.\\n2.1 Preliminary and Problem Formulation\\nComplex task-solving, inherently necessitating\\nmulti-step planning and reasoning, is convention-\\nally formalized as a multi-step decision-making\\nproblem. Historically, complex task-solving was\\npredominantly achieved through human-driven\\nmethods . These methods leveraged human cogni-\\ntive capabilities to determine the suitable action in\\neach step. Formally, considering a complex task\\nq, it is traditionally solved via a sequence of ac-\\ntions (a1, a2,···an), with each action determined'),\n",
       " Document(metadata={'source': './assets-resources/pdfs/human-agent-collab-problem-solving.pdf', 'page': 2}, page_content='by human decision-making, expressed as:\\nat=Human (q, st), (1)\\nwhere st= (a1, o1,···, at−1, ot−1)denotes the\\nhistory information of task state at step tandotis\\nthe observation after at−1is proceeded.\\nThe advent of LLMs has brought a paradigm\\nshift in this arena. Their impressive understand-\\ning and reasoning abilities have prompted research\\ninto LLM-based agents for complex task-solving,\\nthereby enhancing the level of automation in task-\\nsolving. These agent-driven methods (e.g., Re-\\nAct (Yao et al., 2022)), leverage LLM-based agents\\nto supplant human decision-making. This shift is\\nrepresented as:\\nat=Agent (q, st). (2)\\nThis evolution of such AI-driven techniques pro-\\nvides a way to the automation of complex task-\\nsolving.\\nHowever, limited by the current intelligence\\nlevel of LLMs, full automation based on agent-\\ndriven methods is not yet feasible in practical sce-\\nnarios (Kiseleva et al., 2022; Mehta et al., 2023).\\nInspired by autonomous driving (Cui et al., 2024;\\nFu et al., 2024; Bastola et al., 2024), we propose\\nthe problem of LLM-based human-agent collab-\\noration for complex task solving and explore the\\ndynamics and efficacy of the human-agent collab-\\norative methods for complex task solving. We\\nfirst explore a specific form of human-agent col-\\nlaboration: humans intervene in the complex task-\\nsolving process when necessary. Formally, we need\\nto determine whether a human or an agent makes\\ndecisions based on the actions’ complexity and\\ncontextual changes, i.e.,\\nat=Human (q, st)or Agent (q, st),(3)\\nIt is generally perceived that direct human in-\\ntervention in decision-making, particularly in real-\\nworld scenarios, incurs higher costs and diminishes\\nthe system’s automation level (Cai et al., 2023;\\nWang et al., 2023b). On the other hand, human\\nintervention plays an important role in enhancing\\ntask performance and flexibility. Therefore, the\\nobjective of human-agent collaboration is to en-\\nhance the effectiveness of complex task-solving\\nwith minimal reliance on human decision-making.\\nOne key challenge is to determine the stages in\\nthe task-solving process where human interven-\\ntion is most beneficial and effective, aligningwith the goal of minimizing human involvement\\nwhile maximizing task performance .\\n2.2 ReHAC\\nIn this work, we propose a Reinforcement learning-\\nbased Human-Agent Collaboration method, Re-\\nHAC. It formulates the human-agent collabo-\\nration problem as a Markov Decision Process\\n(MDP) framework, represented by the tuple\\n(S,A, P, R, γ ), where Sis the set of states, Ais\\nthe set of actions, P:S×A× Sis the state transi-\\ntion probabilities, Rserves as the reward function,\\nandγthe discount factor.\\nFor each action at∈ A, we define it as a tuple\\n(acollab\\nt, atask\\nt), where acollab\\nt indicates the subtask\\nis allocated to an agent or a human, and atask\\ntis the\\ntask action determined by agent or human:\\nacollab\\nt∼πcollab\\nθ1(acollab\\nt|st)\\natask\\nt∼(\\nπtask\\nθ2(atask\\nt|st),ifacollab\\nt = 0;\\nπtask\\nHuman (atask\\nt|st),otherwise ,\\n(4)\\nwhere πcollab\\nθ1is the collaboration policy model,\\nπtask\\nθ2is the agent-based task policy model, and\\nπtask\\nHuman is the human task policy.\\nTo balance the maximization of task perfor-\\nmance and the cost of human intervention, we de-\\nfine the reward function as:\\nR(s, a) =T(s, a)−λC(s, a), (5)\\nwhere T(s, a)is the measure of expected task re-\\nwards received after taking action ain state s,\\nC(s, a)is the number of human interventions in\\nthe trajectory after taking action a,λis a hyper-\\nparameter that serves as a penalty coefficient of the\\nnumber of human interventions. We utilize Monte-\\nCarlo estimation to compute this reward function.\\nOptimization: Following the REINFORCE algo-\\nrithm (Williams, 1992), we optimize the expected\\nreward:\\nJ(πθ) =Eπθ[R(s, a)], (6)\\nwhich aims to find an optimal policy πθthat ensures\\nthe maximization of task rewards while minimizing\\nthe human intervention costs, and θ= [θ1, θ2].\\nWe utilize the advantage function to enhance the\\nstability of optimization and important sampling'),\n",
       " Document(metadata={'source': './assets-resources/pdfs/human-agent-collab-problem-solving.pdf', 'page': 3}, page_content='for offline learning:\\nA(s, a) =R(s, a)−1\\n|A|X\\na′∈AR(s, a′)\\n∇θJ(πθ) =X\\nsX\\naw(s, a)∇θlogπθ(a|s)A(s, a),\\nw(h, a) =Clip\\x12πθ(s, a)\\nπbeh(s, a)\\x13\\n, (7)\\nwhere A(s, a)is the advantage function, the clip\\nfunction limits the importance sampling term to the\\ninterval 1−ϵto1 +ϵ, and the behavior policy πbeh\\nrepresents the policy under of the offline training.\\nMoreover, we have incorporated an entropy regu-\\nlarization term. This term encourages the policy\\nto explore a variety of actions, thereby preventing\\nthe policy from becoming too deterministic and\\noverfitting to the training data. Finally, the gradient\\nof objective function is as follows:\\n∇θ˜J(πθ) =∇θJ(πθ) +α∇θH(πθ(·|s)).(8)\\n3 Experiments\\n3.1 Experimental Setup\\nDatasets Following Yao et al. (2022); Shinn et al.\\n(2023); Liu et al. (2023b); Xu et al. (2023), we eval-\\nuate the efficacy of our method on question answer-\\ning and coding datasets: (1) HotpotQA (Yang et al.,\\n2018) is a Wikipedia-based question answering\\nbenchmark which needs model to perform multi-\\nhop reasoning over complex questions. (2) Strate-\\ngyQA (Geva et al., 2021) is a question answering\\nbenchmark with questions that need implicit rea-\\nsoning. (3) InterCode (Yang et al., 2023) is an\\ninteractive coding dataset that enables agents to\\nreceive feedback from the code interpreter. In this\\nwork, we use InterCode-SQL part, which requires\\nmodels to write SQL statements to fulfil the query.\\nImplementation details We use LLaMA-2 (Tou-\\nvron et al., 2023) as the collaboration policy model\\nπcollab\\nθ1and use Low-Rank Adaptation (LoRA, Hu\\net al. (2021)) methods to train the policy model.\\nIn all experiments, we utilized ChatGPT (gpt-3.5-\\nturbo-0613) to simulate the agent policy πtask\\nθ2.\\nMore model implementation and data collection\\ndetails can be found in Appendix A.1.\\nIn this study, we set humans and agents to solve\\ntasks under the ReAct framework (Yao et al., 2022)\\nfor question-answering datasets. The action space\\nofataskis {Search[entity], Lookup[keyword], and\\nFinish[answer]}. All actions are supported by aWikipedia web API, following the original Re-\\nAct implementation. For the InterCode dataset,\\nwe solve tasks under the “Try Again” framework\\n(Yang et al., 2023). Here, agents and humans in-\\nteract with the code interpreter through the action\\natand receive execution outputs from the code\\ninterpreter as observations ot. The task-solving\\nprocess ends if any one of the following conditions\\nis satisfied: 1) the Finish[answer] action is exe-\\ncuted actively by πtask\\nθ2for the question answering\\ndataset. 2) the task reward T(s, a) = 1 for Inter-\\nCode dataset. 3) the number of actions texceeds a\\npre-defined step threshold.\\nReward Calculation For all datasets, the final\\nreward is computed as equation (5). For question\\nanswering datasets, we choose the F1 score as the\\ntask reward T(s, a). For the InterCode dataset,\\nfollowing Yang et al. (2023), we use Intersection\\nover Union as the task reward T(s, a).\\nBaselines We compare our method ReHAC with\\nthe following baselines: 1) Agent-only which car-\\nries out all actions by agents. 2) Human-only,\\nwhich conducts all actions by humans. 3) Ran-\\ndom, which selects an agent or human randomly\\nat a probability of 50% to perform each action. 4)\\nPrompt, which prompts the agent to actively decide\\nwhether the action is executed by itself or a human.\\n5) Imitation Learning (IL), which trains the pol-\\nicy model to decide whether the action should be\\nfinished by an agent or human by the IL method.\\nMore details about baselines can be found in the\\nAppendix A.2.\\n3.2 Overall Results\\nIn this section, we verify the effectiveness of our\\nproposed ReHAC method for human-agent collab-\\noration on the HotpotQA dataset.\\nHuman-Agent Experiments Figure 2(a) shows\\nthe evaluation results of human-agent collabora-\\ntion on the HotpotQA dataset. From the figure,\\nwe can observe that all human-agent collabora-\\ntion methods outperform Human-only and Agent-\\nonly methods. This underscores the importance')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:45:45.061062Z",
     "start_time": "2024-10-21T21:45:43.265728Z"
    }
   },
   "source": [
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Add Retriever Tool"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:45:54.046685Z",
     "start_time": "2024-10-21T21:45:54.040972Z"
    }
   },
   "source": [
    "from langchain.tools.retriever import create_retriever_tool"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:46:43.457986Z",
     "start_time": "2024-10-21T21:46:43.453607Z"
    }
   },
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    'retrieve_info_from_papers',\n",
    "    'Search and return information about a paper discussing usage of LLMs for human collaborative problem solving.',\n",
    ")\n",
    "\n",
    "tools = [retriever_tool]"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Define the graph"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:46:45.783063Z",
     "start_time": "2024-10-21T21:46:45.731733Z"
    }
   },
   "source": [
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:54:41.267380Z",
     "start_time": "2024-10-21T21:54:41.125606Z"
    }
   },
   "source": [
    "from typing import Annotated, Literal, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\"\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4o\")\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "print(\"*\" * 20 + \"Prompt[rlm/rag-prompt]\" + \"*\" * 20)\n",
    "prompt = hub.pull(\"rlm/rag-prompt\").pretty_print()  # Show what the prompt looks like"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Prompt[rlm/rag-prompt]********************\n",
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: \u001B[33;1m\u001B[1;3m{question}\u001B[0m \n",
      "Context: \u001B[33;1m\u001B[1;3m{context}\u001B[0m \n",
      "Answer:\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:54:44.766053Z",
     "start_time": "2024-10-21T21:54:44.758398Z"
    }
   },
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "retrieve = ToolNode([retriever_tool])\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\n",
    "    \"generate\", generate\n",
    ")  # Generating a response after we know the documents are relevant\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:54:49.579721Z",
     "start_time": "2024-10-21T21:54:48.331996Z"
    }
   },
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ],
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T21:58:40.742822Z",
     "start_time": "2024-10-21T21:58:36.915224Z"
    }
   },
   "source": [
    "import pprint\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"In this paper how do the authors set up the collaboration between the human and the LLMs?\"),\n",
    "    ]\n",
    "}\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "\"Output from node 'agent':\"\n",
      "'---'\n",
      "{ 'messages': [ AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_AnjUdVEwF9JBHyUmLnrB0x4k', 'function': {'arguments': '{\"query\":\"collaboration between human and LLMs\"}', 'name': 'retrieve_info_from_papers'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7'}, id='run-b9c9b98c-d021-4dc8-b385-4f1321ab304a-0', tool_calls=[{'name': 'retrieve_info_from_papers', 'args': {'query': 'collaboration between human and LLMs'}, 'id': 'call_AnjUdVEwF9JBHyUmLnrB0x4k', 'type': 'tool_call'}])]}\n",
      "'\\n---\\n'\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "\"Output from node 'retrieve':\"\n",
      "'---'\n",
      "{ 'messages': [ ToolMessage(content='narios where high accuracy is crucial, such as the\\nlegal or financial domains. Addressing this chal-\\nlenge extends beyond just enhancing the agents’\\ncapabilities. Incorporating human intuition and\\nwisdom is equally vital for the effective manage-\\nment of these intricate and evolving tasks, offering\\na complementary approach to the limitations of\\ncurrent agent technologies.\\nIn this work, we introduce the problem of LLM-\\nbased human-agent collaboration for complex\\ntask solving , aiming to augment the capabilities of\\nLLM-based agents by integrating human intuition\\nand wisdom. The idea is analogous to the evolu-\\ntion in autonomous driving technology, which has\\nbeen categorized into varying levels of autonomy,\\nranging from no automation, conditional automa-\\ntion to full automation (Khan et al., 2022; SAE\\nInternational, 2021). Referring to this framework,\\nwe define the different levels of human-agent col-\\nlaboration, as illustrated in Figure 1. Applying\\nthis conditional automation mode to LLM-based\\nagents offers a practical path for their deployment\\nin real-world scenarios, acknowledging the current\\nlimitations in their cognitive capabilities. Instead\\nof aiming for full automation, human-agent col-\\nlaboration under the paradigm of conditional au-\\ntomation enables humans to intervene the complex\\ntask-solving when necessary, while agents handle\\nmost of the sub-tasks. This takes advantage of both\\nhuman and machine intelligence.\\nWhile advancements in LLMs significantly en-\\nhance the capacity for mutual understanding in\\nhuman-agent collaboration, several crucial chal-\\nlenges persist. These challenges include defining\\nthe division of labor between humans and agents,\\ndetermining the granularity of tool execution, man-\\naging proactive interruption, and implementing\\nmulti-level intervention. However, our research\\nspecifically focuses on scenarios where humans\\ndirectly replace agents in action. The key chal-\\nlenge we aim to address in human-agent collab-\\noration lies in determining the optimal stages for\\nhuman intervention in task-solving and minimiz-\\ning such intervention to enhance efficiency. Some\\nresearchers have made preliminary attempts, by de-\\nsigning heuristic rules or specialized prompts to\\ndetermine the stages at which agents should seek\\nhuman assistance (Cai et al., 2023; Wu et al., 2022a;\\nMehta et al., 2023; Wang et al., 2023b). However,\\nthese rule-based or prompt-driven approaches are\\nheavily reliant on specific application contexts andlack universality. They often demand a deep under-\\nstanding of the domain and substantial experience\\nfrom the designers, otherwise, suboptimal design\\nchoices can lead to reduced performance. Apart\\nfrom that, a standardized formal framework and\\nuniversally accepted paradigm for leveraging large\\nlanguage models (LLMs) in human-agent collabo-\\nration is still lacking.\\nTo overcome the aforementioned challenges, we\\npropose a Reinforcement Learning-based Human-\\nAgent Collaboration method, ReHAC , aimed at\\neffectively combining human intervention with the\\nautomation capabilities of LLM-based agents. Our\\nmethod, leveraging reinforcement learning, trains\\na policy model to dynamically identify the most\\nadvantageous moments for human input during the\\ntask-solving process. ReHAC is a learnable gen-\\neral framework that can be applied to various sce-\\nnarios and does not require additional prior knowl-\\nedge to design rules and prompts. For training\\nthis policy model, we collect a dataset compris-\\ning tasks collaboratively completed by humans and\\nLLM-based agents, utilized for the offline training\\nof the policy model. We conducted extensive ex-\\nperiments on three multi-step reasoning datasets:\\nHotpotQA, StrategyQA, and InterCode, using two\\npopular LLM-based agent frameworks, ReAct and\\n\"Try-again\". The experimental results indicate that\\nwith a policy model learned from limited data, Re-\\nHAC can effectively allocate human intervention\\nin human-agent collaboration scenarios, thereby\\n\\nWu et al., 2022a; Mehta et al., 2023). Further-\\nmore, there is an increasing emphasis on develop-\\ning specialized prompts that motivate LLM-based\\nagents to proactively seek human input, thus nurtur-\\ning a more interactive and collaborative dynamic\\nin these partnerships (Huang et al., 2022; Wang\\net al., 2023b). However, the effectiveness of these\\nmethods relies on designing high-quality rules or\\nprompts. This is highly dependent on the designer’s\\ndomain knowledge. Poor design may result in a sys-\\ntem that cannot accurately understand or respond to\\ncomplex task requirements. Our research focuses\\non designing a generalised and learnable method\\nthat coordinates human to effectively work with\\nLLM-based agents in the form of direct planning.\\n6 Conclusion\\nIn this paper, we propose the problem of large\\nlanguage model-based human-agent collaboration,\\ndelving into the synergy of human intuition and\\nexpertise with the computational prowess of LLM-\\nbased agents, particularly emphasizing their appli-\\ncation in intricate decision-making tasks. We intro-\\nduce a reinforcement learning-based approach for\\nhuman-agent collaboration, named ReHAC. Cen-\\ntral to ReHAC is a learnable policy model designed\\n\\nLarge Language Model-based Human-Agent Collaboration\\nfor Complex Task Solving\\nXueyang Feng1,2∗, Zhi-Yuan Chen1,2∗, Yujia Qin3, Yankai Lin1,2†\\nXu Chen1,2†, Zhiyuan Liu3, Ji-Rong Wen1,2\\n1Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China\\n2Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China\\n3Department of Computer Science and Technology, Tsinghua University, Beijing, China\\n{xueyangfeng, zhiyuanc2001, yankailin, xu.chen}@ruc.edu.cn\\nAbstract\\nIn recent developments within the research\\ncommunity, the integration of Large Language\\nModels (LLMs) in creating fully autonomous\\nagents has garnered significant interest. De-\\nspite this, LLM-based agents frequently demon-\\nstrate notable shortcomings in adjusting to dy-\\nnamic environments and fully grasping hu-\\nman needs. In this work, we introduce the\\nproblem of LLM-based human-agent collab-\\noration for complex task-solving, exploring\\ntheir synergistic potential. In addition, we pro-\\npose a Reinforcement Learning-based Human-\\nAgent Collaboration method, ReHAC . This\\napproach includes a policy model designed to\\ndetermine the most opportune stages for hu-\\nman intervention within the task-solving pro-\\ncess. We construct a human-agent collabo-\\nration dataset to train this policy model in\\nan offline reinforcement learning environment.\\nOur validation tests confirm the model’s ef-\\nfectiveness. The results demonstrate that the\\nsynergistic efforts of humans and LLM-based\\nagents significantly improve performance in\\ncomplex tasks, primarily through well-planned,\\nlimited human intervention. Datasets and code\\nare available at: https://github.com/\\nXueyangFeng/ReHAC .\\n1 Introduction\\nIn today’s increasingly complex world, humans are\\nconfronted with multifaceted tasks stemming from\\ntechnical, social, and economic domains. Solv-\\ning these complex tasks necessitates not only hu-\\nman interaction with the environment but also in-\\ntricate decision-making processes. To alleviate\\nhuman workload and enhance the automation of\\ntasks in both professional and personal spheres, re-\\nsearchers have been actively developing advanced\\ntools for human assistance (Zawacki-Richter et al.,\\n∗Equal Contribution. The order is determined by dice\\nrolling.\\n†Corresponding Authors.\\nAgent Human Env\\nAllocator EnvEnv\\n(a)\\nAct Act(b)\\n(c)Agent\\nHumanAct\\nAct\\nI need  helpFigure 1: Different Levels of Automation. (a) No au-\\ntomation: Tasks are entirely performed by humans. (b)\\nFull automation: Tasks are completely executed by\\nagents without human intervention. (c) Conditional\\nautomation: Humans are required only for specific sub-\\ntasks, without continuous monitoring.\\n2019; Amershi et al., 2019). Recently, the emer-\\ngence of Large Language Models (LLMs) such\\nas LLaMA (Touvron et al., 2023), Gemini (Team\\net al., 2023) and GPT (Brown et al., 2020; Achiam\\net al., 2023) has marked a significant milestone.\\nLLMs’ remarkable abilities in task understanding,\\nplanning, and reasoning (Zhao et al., 2023b) have\\ngiven rise to the development of LLM-based au-\\ntonomous agents (Wang et al., 2023a; Yao et al.,\\n2022; Shinn et al., 2023). These agents are de-\\nsigned to leverage the LLMs’ capabilities to assist\\nhumans in solving complex tasks autonomously.\\nThe LLMs’ capabilities enable them to effectively\\nnavigate and address the complexities encountered\\nin real-world scenarios, thereby offering substan-\\ntial support in human decision-making processes\\nof task-solving.\\nDespite the remarkable progress of LLM-based\\nagents, there remains a notable gap in their intelli-\\ngence level to handle complex and dynamic real-\\nworld tasks with human-like proficiency. This limi-\\ntation poses a significant challenge to their practi-\\ncality in real-world applications, especially in sce-arXiv:2402.12914v1  [cs.CL]  20 Feb 2024\\n\\navailable information. Thus, our ReHAC method\\nreturns a correct response “Seven Days Battles”.\\nThis case also highlights an insightful aspect of our\\nresearch into LLM-based agents: Researchers are\\ncommitted to eliminating hallucinations in large\\nlanguage models (LLMs) to create rigorous and\\naccurate intelligent agents. However, many tasks\\nrequire imagination and intuition, making it cru-\\ncial to integrate human creative thinking through\\nhuman-agent collaboration at this juncture.\\n4 Discussion\\nIn this paper, we conduct a preliminary exploration\\nof key aspects of human-agent collaboration, aim-', name='retrieve_info_from_papers', id='d2f72b01-2d5e-463b-934d-6bda2af6728e', tool_call_id='call_AnjUdVEwF9JBHyUmLnrB0x4k')]}\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jzhu/PycharmProjects/vidhya-agents-course/venv/lib/python3.12/site-packages/langsmith/client.py:333: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Output from node 'generate':\"\n",
      "'---'\n",
      "{ 'messages': [ 'The authors set up the collaboration between humans and LLMs '\n",
      "                'by proposing a Reinforcement Learning-based Human-Agent '\n",
      "                'Collaboration method called ReHAC. This method includes a '\n",
      "                'policy model to determine the optimal stages for human '\n",
      "                'intervention in task-solving processes. The approach aims to '\n",
      "                'combine human intuition and expertise with the computational '\n",
      "                'capabilities of LLM-based agents effectively.']}\n",
      "'\\n---\\n'\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T22:00:27.822109Z",
     "start_time": "2024-10-21T22:00:24.135511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = graph.invoke(inputs)\n",
    "\n",
    "output['messages'][-1].content"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jzhu/PycharmProjects/vidhya-agents-course/venv/lib/python3.12/site-packages/langsmith/client.py:333: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The authors set up the collaboration between humans and LLMs by proposing a Reinforcement Learning-based Human-Agent Collaboration method called ReHAC. This method includes a policy model to determine the optimal stages for human intervention during task-solving. The approach aims to combine human intuition with the automation capabilities of LLM-based agents effectively.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
